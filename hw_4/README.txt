For this assignment, I have used multiprocessing and dask to optimize/parallelize the running of a pi approximation script that is defined in the first jupyter cell. In order to see true optimization, I broke down the iterations of the pi-simulation into chunks which I then ran in parallel. However, I had to change the chunksize in order to limit how many processes the managers ran, which resulted in several noticeable jumps in simulation rate and total simulation time.

The designation for chunksize and when to change chunksize was fairly arbitrary and the code could immediately be improved if those numbers are played with. Regardless, the graph produced at the end is logical and all of the features can be explained. The simple simulation is fairly linear and, although it takes less for shorter simulations, ends up being significantly slower beyond about 10^5 darts being thrown. On the other hand, the multiprocessing is somewhat slower initially but after the flat setup time is overcome, takes uniformly less time than the simple simulation. On the otherhand, the dask simulation/computation has a much larger setup time but passes both other simulations after just 5*10^5 darts thrown. Notice the jumps at 10^3 darts and 10^4 darts thrown which is when the chunksize is changed.

Thanks for reading! I actually had a lot of fun doing this!
